{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are bouts of activite at are long with large tails and heads. When we window them we might end up with windows that are just he head or tail. These windows dont really reprisent the action. So here we will look athe the # of wrongly pred windows in the firs 30% vs. the # wrong windows in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "import json\n",
    "\n",
    "\n",
    "#if we want the tails to be the first 15% and last 15%\n",
    "percent_head = 0.05\n",
    "percent_tail = 0.05\n",
    "\n",
    "path_to_model_def = '/home/kuba/Projects/MedicationTakingData/resmodel' #this is were the .py file is \n",
    "path_to_dir_with_model_pt_file = '/home/kuba/Projects/MedicationTakingData/resmodel/res_search_00/res_search_00_7'\n",
    "\n",
    "#the watch and recoding we willbe evaling\n",
    "WATCH_DIR = '/home/kuba/Documents/Data/Raw/Listerine/3_final/16'\n",
    "recording = '2024-03-24_13_10_54'\n",
    "\n",
    "HERTZ = 100\n",
    "ACTIVITY_NAME_TO_CLASS_INDEX_MAPPING = {\n",
    "    'water':0,\n",
    "    'listerine':1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will get the bouts then we will split them up into windows \n",
    "def get_bouts(recording_dir):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    all_starts = []\n",
    "    all_ends = []\n",
    "    \n",
    "    label_mapping = [\n",
    "        ('left', 'water', 0.0),\n",
    "        ('left', 'listerine', 1.0),\n",
    "        ('right', 'water', 0.0),\n",
    "        ('right', 'listerine', 1.0)\n",
    "    ]\n",
    "    \n",
    "    with open(os.path.join(recording_dir, 'labels.json')) as f:\n",
    "        labels = json.load(f)\n",
    "        \n",
    "    acc = pd.read_csv(os.path.join(recording_dir, 'acceleration.csv'), skiprows=1)\n",
    "    gyro = pd.read_csv(os.path.join(recording_dir, 'gyroscope.csv'), skiprows=1)\n",
    "    \n",
    "    #convert the timestap to sec\n",
    "    acc['timestamp'] = (acc['timestamp'] - acc['timestamp'].iloc[0]) * 1e-9\n",
    "    gyro['timestamp'] = (gyro['timestamp'] - gyro['timestamp'].iloc[0]) * 1e-9\n",
    "    \n",
    "    merged_data = pd.merge_asof(acc, gyro, on='timestamp', \n",
    "                                suffixes=('_acc', '_gyro'))\n",
    "    \n",
    "    for side, liquid, label_value in label_mapping:\n",
    "        if side in labels and liquid in labels[side]:\n",
    "            for bout in labels[side][liquid]:\n",
    "                start_time = bout['start'] \n",
    "                end_time = bout['end']\n",
    "                # Extract data for this bout\n",
    "                bout_data = merged_data[(merged_data['timestamp'] >= start_time) & (merged_data['timestamp'] <= end_time)].copy()\n",
    "                \n",
    "                start_index = (merged_data[\"timestamp\"] < start_time).sum()\n",
    "                end_index = start_index + len(bout_data) \n",
    "\n",
    "                if len(bout_data) > 0:\n",
    "                    all_data.append(bout_data)\n",
    "                    all_labels.append(label_value)\n",
    "                    all_starts.append(start_index)\n",
    "                    all_ends.append(end_index)\n",
    "\n",
    "    return all_data, all_labels, all_starts, all_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are given a specific start and stop then we find the window data and add too list \n",
    "# we also see if its in the head or tail then add it too that list too\n",
    "def preprocess_window(df, start, window_size, percent_head, percent_tail):\n",
    "    window = df.iloc[start:start + window_size]\n",
    "    X_acc = torch.tensor([window[col].values for col in ['x_acc', 'y_acc', 'z_acc']], dtype=torch.float32)\n",
    "    X_gyro = torch.tensor([window[col].values for col in ['x_gyro', 'y_gyro', 'z_gyro']], dtype=torch.float32)\n",
    "    all_windows = torch.cat((X_acc, X_gyro), dim=0).unsqueeze(0)\n",
    "    \n",
    "    # Calculate bout length and threshold positions\n",
    "    bout_length = len(df)\n",
    "    head_threshold = int(bout_length * (percent_head ))\n",
    "    tail_threshold = bout_length - int(bout_length * (percent_tail))\n",
    "    \n",
    "\n",
    "    # print(bout_length)\n",
    "    # print(head_threshold)\n",
    "    # print('tail htresh', tail_threshold)\n",
    "\n",
    "\n",
    "\n",
    "    # Check if window is within head or tail thresholds\n",
    "    window_end = start + window_size\n",
    "    is_in_head = start < head_threshold\n",
    "    is_in_tail = window_end > tail_threshold\n",
    "    \n",
    "    filtered_windows = all_windows if (is_in_head or is_in_tail) else None\n",
    "    \n",
    "    return all_windows, filtered_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(bout, label, window_size, stride, percent_head, percent_tail):\n",
    "    assert 'timestamp' in bout.columns, \"Sensor data must include 'timestamp' column.\"\n",
    "    # y_df = pd.DataFrame(label, columns=['labels'])\n",
    "    # df = pd.concat([bout, y_df], axis=1)\n",
    "    X = []\n",
    "    X_percent = []\n",
    "    y = []\n",
    "    y_percent = []\n",
    "\n",
    "    #where just move through all posible window vals then send them over to get proccessed\n",
    "    #if they are valed we willhave the window dat back\n",
    "    for i in range(0, len(bout) - window_size + 1, stride):\n",
    "        x, x_percent = preprocess_window(bout, i, window_size, percent_head, percent_tail) \n",
    "        X.append(x)\n",
    "        y.append(label)\n",
    "        if x_percent is not None:\n",
    "            X_percent.append(x_percent)\n",
    "            y_percent.append(label)\n",
    "\n",
    "    \n",
    "    X = torch.cat(X, dim=0) if X else torch.tensor([])\n",
    "    X_percent = torch.cat(X_percent, dim=0) if X_percent else torch.tensor([])\n",
    "    return X, X_percent, torch.tensor(y, dtype=torch.float32), torch.tensor(y_percent, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(dir):\n",
    "    txt_path = os.path.join(dir, f'desc.txt')\n",
    "    with open(txt_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    dic = eval(content)\n",
    "    return dic['window_size'], dic['stride'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_wrong(model, device, X, y):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = torch.sigmoid(model(X))\n",
    "        predictions = torch.argmax(out, dim=1)\n",
    "\n",
    "        return (predictions != y).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuba/Projects/MedicationTakingData/resmodel/res_search_00/res_search_00_7/res_search_00_7_bestF1.pth\n",
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(path_to_model_def)\n",
    "head_tail = os.path.split(path_to_dir_with_model_pt_file)\n",
    "model_path = os.path.join(path_to_dir_with_model_pt_file, f'{head_tail[1]}_bestF1.pth')\n",
    "\n",
    "\n",
    "#get model and meta data\n",
    "window_size, stride = read_txt(path_to_dir_with_model_pt_file)\n",
    "print(model_path)\n",
    "\n",
    "# load model\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "recording_dir = f'{WATCH_DIR}/{recording}'\n",
    "\n",
    "\n",
    "all_bouts, all_labels, all_starts, all_ends = get_bouts(recording_dir)\n",
    "\n",
    "total_wrong = 0\n",
    "total_wrong_subset = 0\n",
    "\n",
    "for bout, label in zip(all_bouts, all_labels):\n",
    "    X, X_percent, y, y_percent = get_windows(bout=bout, label=label, window_size=window_size, stride=stride, percent_head=percent_head, percent_tail=percent_tail)\n",
    "\n",
    "    # print('X percent shape is ',X_percent.shape, 'and shape of X is: ',X.shape)\n",
    "    all_wrong = get_num_wrong(model=model, device=device, X=X, y=y)\n",
    "    subsset_wrong = get_num_wrong(model=model, device=device, X=X_percent, y=y_percent)\n",
    "\n",
    "    total_wrong += all_wrong\n",
    "    total_wrong_subset += subsset_wrong\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "print(total_wrong)\n",
    "print(total_wrong_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
